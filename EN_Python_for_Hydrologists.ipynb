{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief introduction to Python for Hydrological Analyses\n",
    "<br>\n",
    "<img style=\"float: left; padding-right: 15px; padding-left: 0px;\" src=\"source/logo_flood_proofs.png\" width=\"260px\" align=”left” >\n",
    "\n",
    "<div style=\"text-align: justify\">This is a Jupyter Notebook, a web-based interactive development environment that allows to create and share python codes.\n",
    "First things first, what is **Python**? Python is an high-level and general-purpose programming language. It can be used to write software in a wide variety of application domains, including hydrology. Python can be used to perform numerical calculations, statistical analyses or to access and plot data (even large datasets). <br>\n",
    "In Jupyter Notebook the *Python shell* is embedded. The shell is where you can write and execute a line (or multiple lines) of code.\n",
    "Python is open-source, and several packages are available covering many scientific and technological fields.</div>\n",
    "\n",
    "Let's start using Python as a **calculator**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.333333333333336"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "190/3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed we can assign this result to a variable, and use the variable for further math or for other operations (such as converting to integer and priting it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 190/3\n",
    "new_result = result - 14\n",
    "int_result = int(new_result)\n",
    "print(int(int_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to perform some more complex calculus? We can import the **math** package, loading several mathematical functions (such as the square root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "sqrt_result = math.sqrt(int_result)\n",
    "print(sqrt_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to work not with a single value, but with a **vector** composed of multiple values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "array = [1,4,100,3,-2]\n",
    "print(array)\n",
    "print('------> complete array maximum: ' + str(np.max(array)))\n",
    "array_nan = array\n",
    "array_nan[2] = np.nan\n",
    "print(array_nan)\n",
    "print('------> incomplete array maximum: ' + str(np.max(array)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the last result correct? Shouldn't be 4 the new maximum value? We can use a specific function for accounting for \"Nan\" or missing values: *np.nanmax* (part of numpy package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------> incomplete array maximum: ' + str(np.nanmax(array)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Timeseries\n",
    "Can we generate **timeseries** (multiple values with associated date) and plot it? Of course!\n",
    "Let's start by importing some useful libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import wget\n",
    "import os\n",
    "\n",
    "# We can also define personal function, this one for example allow to visualize the output of dropdowns menus\n",
    "def on_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            print(\"Selected value: \" + change['new'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed with a random timeseries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.Series(np.random.randn(365), index=pd.date_range('1/1/2010', periods=365))\n",
    "plt.figure()\n",
    "ts.plot(style='b-', label='Random timeseries')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most interesting is to work with existing timeseries. The NOAA provides a global dataset of daily rainfall and temperature values, the **Global Historical Climate Network Daily** (https://www.ncdc.noaa.gov/ghcn-daily-description).\n",
    "The dataset can be queried by country by selecting it from the following dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of available countries and generate dropdown selector\n",
    "with open('source/ghcnd-countries.txt', 'r') as file:\n",
    "    list_country = [line for line in file]\n",
    "country_chooser = widgets.Dropdown(\n",
    "    options=['Choose a country'] + list_country,\n",
    "    value='Choose a country',\n",
    "    description='Country:',\n",
    "    disabled=False,\n",
    ")\n",
    "country_chooser.observe(on_change)\n",
    "display(country_chooser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing the next piece of code a dropdown menu of the available station for the selected country will be show. Please, select the station of interest.\n",
    "**NB!** Some countries (Brazil, Australia and US) have too many station and can break the system, for those countries manual selection of the station is feasible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of the available stations\n",
    "country_code = country_chooser.value[0:2]\n",
    "list_stations = pd.read_fwf('source/ghcnd-stations.txt',\n",
    "                            widths=[2,9,9,10,7,4,31,3,10],\n",
    "                            header=None, usecols=[0,1,2,3,4,6], \n",
    "                            names=['COUNTRY','CODE','LAT','LON','ELEV','NAME'])\n",
    "list_stations_in = list_stations.loc[list_stations['COUNTRY']==country_code].sort_values('NAME', ascending=True)\n",
    "\n",
    "if len(list_stations_in)<4000:\n",
    "    station_chooser = widgets.Dropdown(\n",
    "        options=['Choose a station'] + list(list_stations_in['COUNTRY'] + list_stations_in['CODE'] + ' ' + list_stations_in['NAME']),\n",
    "        value='Choose a station',\n",
    "        description='Station:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    station_chooser.observe(on_change)\n",
    "    display(station_chooser)\n",
    "else:\n",
    "    print('Station list is too long! Please, manually choose the station code from the available list at the web address https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt among the ones with first column starting with ' + country_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code will download the series from the noaa series and analyse the available variables. \n",
    "Choose the variable for the analysis from the dropdown menu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert information only for manually selecting the station code (for Brazil, Austalia and US)\n",
    "section_code =  None     # es: 'BR00B7-0400'\n",
    "section_name = None      # es: 'SAO JOAO DE IRACEMA'\n",
    "###############################################################################################\n",
    "\n",
    "if section_code is None:\n",
    "    section_code = station_chooser.value.split(' ', 1)[0]\n",
    "    section_name = station_chooser.value.split(' ', 1)[1]\n",
    "file_name = section_code +'.csv'\n",
    "out_path = 'meteo/' + file_name\n",
    "\n",
    "# Check if the file has been already downloaded\n",
    "if os.path.isfile(out_path):\n",
    "    print('Section ' + section_code + ' ' + section_name + ' already downloaded!')\n",
    "    print('DONE!')\n",
    "else:\n",
    "    print('Dowloading section ' + section_code + ' ' + section_name + '... It can take some times!')\n",
    "    https_address = 'https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/'\n",
    "    wget.download(https_address + file_name, out = out_path)\n",
    "    print('DONE!')\n",
    "    \n",
    "# Open the file and analyse the available variables\n",
    "info_station = pd.read_csv(out_path, header=0, usecols=['STATION','NAME','LATITUDE','LONGITUDE','ELEVATION'], nrows=1)\n",
    "full_series = pd.read_csv(out_path, header=0, index_col='DATE', parse_dates=True, usecols=lambda c: c in {'DATE','PRCP','SNWD','TMAX','TMIN','TAVG'}) #, usecols=[1,2,3], names=['date','type','val'])\n",
    "dic_vars={'precipitation':['PRCP', 'Rainfall(mm)'], 'temperature mean':['TAVG','Temperature(°C)'], 'temperature max':['TMAX','Temperature(°C)'], 'temperature min':['TMIN','Temperature(°C)'], 'snow depth':['SNWD', 'Snow depth(cm)']}\n",
    "\n",
    "available_vars = [i for i in dic_vars if dic_vars[i][0] in full_series.columns]\n",
    "\n",
    "var_chooser = widgets.Dropdown(\n",
    "    options=['Choose a variable'] + available_vars,\n",
    "    value='Choose a variable',\n",
    "    description='Vars available:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "var_chooser.observe(on_change)\n",
    "display(info_station.style.hide_index())\n",
    "display(var_chooser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can interactively plot one of the available timeseries by choosing the station, the variable and also the time limits: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert information only for choosing a sub-period of the whole series\n",
    "time_start = None    # Set a date in the format 'YYYY-MM-DD' or None for plot the series from the beginning\n",
    "time_end = None      # Set a date in the format 'YYYY-MM-DD' or None for plot the series up to the end\n",
    "####################################################################\n",
    "\n",
    "# Read data series\n",
    "variable = var_chooser.value\n",
    "temp_series = full_series[[dic_vars[variable][0]]]/10\n",
    "\n",
    "# Set time range\n",
    "time_start = temp_series.first_valid_index() if time_start is None else pd.to_datetime(time_start,format='%Y-%m-%d')\n",
    "time_end = temp_series.last_valid_index() if time_end is None else pd.to_datetime(time_end,format='%Y-%m-%d')\n",
    "if time_start > time_end:\n",
    "    raise ValueError(\"time_start is larger than time_end, verify your data!\")\n",
    "time_range = pd.date_range(time_start,time_end,freq='1D')\n",
    "temp_series = temp_series.reindex(time_range)\n",
    "\n",
    "display(temp_series)\n",
    "\n",
    "# Manage plot\n",
    "ax = temp_series.plot(style='b', title=variable + ' at ' + section_name, figsize=(15,5))\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(dic_vars[variable][1])\n",
    "ax.get_legend().remove()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series can be asily managed with python for resampling and statistical operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resample frequency can be set, e.g., to annual 'Y' or monthly 'M'\n",
    "temp_resampled_max = temp_series.reindex(time_range).resample('M').max()\n",
    "temp_resampled_min = temp_series.reindex(time_range).resample('M').min()\n",
    "temp_resampled_avg = temp_series.reindex(time_range).resample('M').mean()\n",
    "\n",
    "# Manage plot\n",
    "ax = temp_resampled_max.plot(style='r', title=variable + ' at ' + section_name, figsize=(15,5))\n",
    "temp_resampled_min.plot(style='b',ax=ax)\n",
    "temp_resampled_avg.plot(style='g',ax=ax)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(dic_vars[variable][1])\n",
    "plt.legend(['max','min','avg'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flood frequency analysis using Python\n",
    "We can use python to compute flood statistics on a discharge timeseries. Reference to *hydro-informatics.github.io* <br>\n",
    "\n",
    "Occurence of relevant (extreme) flood events can be expressed as **return period**, expressing the average recurrence interval of an event of a certain magnitude in units of time. It is the inverse of the **exceedance probability** (the likelihood of an event of a certain magnitude or higher).<br>\n",
    "A significant assumption in calculating the return period is that individual events are assumed indipendent. This means that, for any given year, the probability of a 100-year flood occurring is 1/100.\n",
    "Here below a table showing the recurrence intervals and related probabilities of occurrences.\n",
    "\n",
    "| Return Period (years) | Annual exceeding probability (%) |\n",
    "| --- | --- |\n",
    "| 2 | 50 |\n",
    "| 5 | 10 |\n",
    "| 10 | 10 |\n",
    "| 50 | 2 |\n",
    "| 100 | 1 |\n",
    "| 500 | 0.2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we should import some useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import wget\n",
    "import os\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we should import the **discharge data**. \n",
    "Let's see what \"txt\" series files can be find inside the \"discharge\" folder.\n",
    "Further data can be downloaded from the GRDC data portal by performing a custom request (https://portal.grdc.bafg.de/applications/public.html?publicuser=PublicUser#dataDownload/Stations). The request will be evaluated by the data provider and in not much time an email with a download url will be provided. \n",
    "The url can be inserted and diectly analysed with this tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Modify this section by inserting the download address provided by the GRDC website for download more data\n",
    "# es: download_link = 'https://portal.grdc.bafg.de/grdcdownload/external/53c13313-e359-4f61-bb9d-d803b2ab74e1/2021-07-01_16-52.zip'\n",
    "download_link = None\n",
    "############################################################################################################\n",
    "\n",
    "if not download_link is None:\n",
    "    os.makedirs('temp', exist_ok=True)\n",
    "    wget.download(download_link, out= 'temp/')\n",
    "    with zipfile.ZipFile('temp/' + os.path.basename(download_link), 'r') as zip_ref:\n",
    "        zip_ref.extractall('discharge/')\n",
    "    \n",
    "files = glob.glob(\"discharge/*.txt\")\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list don't tell us much abouyt the file content, we can open one of them to understand the content of each file \n",
    "\n",
    "**NOTE! Python numbering starts from 0!!**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read a preview of the file\n",
    "number_of_lines = 40\n",
    "\n",
    "with open(files[0],'rb') as file:\n",
    "    for i in np.arange(0,number_of_lines,1): \n",
    "        line = file.readline().decode('ISO-8859-1')\n",
    "        print(str(i) + ' ' + line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines between 8 and 18 of each file contains all the information about the station, we can use python capability of manage different file type to summarize those information in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the 11 lines after line 8 (Python numbering starts from 0!)\n",
    "for ind, file in enumerate(files,0):\n",
    "    data = pd.read_csv(file, skiprows=8, nrows=11, sep=\":\", encoding='ISO-8859-1', header=None, names=['cod','val'])    \n",
    "    if ind == 0:\n",
    "        list_vars = [i.replace('# ','') for i in data['cod']]\n",
    "        df_stations = pd.DataFrame(index=np.arange(0,len(files),1),columns=list_vars)\n",
    "    data['cod'] = list_vars\n",
    "    data = data.set_index(['cod'])\n",
    "    for var in list_vars:\n",
    "        df_stations.loc[ind][var] = data.loc[var].values[0].strip()\n",
    "\n",
    "df_stations = df_stations.set_index([\"GRDC-No.\"])\n",
    "display(df_stations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now choose which station to analyse by providing its code to identify the related file, let's start, for example, with the **AWASH WENZ at MELKA KUNTIRE** (COD: 1577100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please, specify an available station code\n",
    "station_code = '1577100'\n",
    "##############################################################################################\n",
    "\n",
    "# Read data from line 37\n",
    "df = pd.read_csv(\"discharge/\" + station_code + \"_Q_Day.Cmd.txt\",\n",
    "                 header=None,\n",
    "                 sep=\";\",\n",
    "                 skiprows=37,\n",
    "                 names=[\"Date\", \"Time\",\"Q\"],\n",
    "                 parse_dates=[0],\n",
    "                 index_col=[\"Date\"])\n",
    "df['Q']=df['Q'].astype(float)\n",
    "ax = df.plot(title=df_stations.loc[station_code][\"River\"] + \" at \" + df_stations.loc[station_code][\"Station\"], figsize=(15,5))\n",
    "ax.set_ylabel('Q (m3/s)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are null values in the series, that corresponds to null values, we can mange them by repalacing with \"np.nan\" that is the standard numpy null value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace negative values with \"null\"\n",
    "df.loc[df['Q']<0,'Q']=np.nan\n",
    "ax = df.plot(title=df_stations.loc[station_code][\"River\"] + \" at \" + df_stations.loc[station_code][\"Station\"], figsize=(15,5))\n",
    "ax.set_ylabel('Q (m3/s)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the complete timeseries: we have to select only the **yearly maxima**. It is quite straightforward with *pandas dataframe*, we can resample our dataset (which has been indexed with dates).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample using the annual maximum value\n",
    "df_ymax = df.resample(\"Y\").max()\n",
    "df_ymax[\"year\"] = df_ymax.index.year\n",
    "df_ymax.reset_index(inplace=True, drop=True)\n",
    "df_ymax = df_ymax.dropna()\n",
    "print(df_ymax)\n",
    "\n",
    "# Manage plot\n",
    "df_ymax.plot(kind='scatter',x='year',y='Q',color='red', figsize=(15,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return period analysis\n",
    "We should compute the exceedence probability *Pr*, and the resulting recurrence interval.\n",
    "Pr is defined as: $Pr_{i} = \\frac{(n-i+1)}{n+1}$\\\n",
    "Where *n* is the total number of observation years and *i* is the rank of the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort in increasing order\n",
    "df_ymax_sorted = df_ymax.sort_values(by=\"Q\")\n",
    "n = df_ymax_sorted.shape[0]\n",
    "df_ymax_sorted.insert(0, \"rank\", range(1, 1 + n))\n",
    "print(df_ymax_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **exceedence probability** ( *pr* ) can be calculated applying the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ymax_sorted[\"pr\"] = (n - df_ymax_sorted[\"rank\"] + 1) / (n + 1)\n",
    "print(df_ymax_sorted.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **recurrence interval**( *return-period* ) is the inverse of the probability, thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ymax_sorted[\"return-period\"] = 1 / df_ymax_sorted[\"pr\"]\n",
    "print(df_ymax_sorted.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once create the table (*dataframe*) with all required information (**Probability** and **Return-Period**) we might plot it to visualise the recurrence interval of each observed discharge. It is worth mentioning that this analysis and the resulting plot refer only to observed values.<br>\n",
    "To extrapolate recurrence interval beyond the observation period (the 1-in-100 years flood values, for instance) a prediction model is needed (Gumbel, GEV, etc..)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ymax_sorted.plot.scatter(y=\"Q\",\n",
    "                         x=\"return-period\",\n",
    "                         title=\"Return period [years] \",\n",
    "                         color='blue',\n",
    "                         grid=True,\n",
    "                         fontsize=14,\n",
    "                         logy=False,\n",
    "                         label=\"Sorted values\",\n",
    "                         figsize=(15,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That is for this brief practical introduction to Python! \n",
    "### You might exercise a bit:\n",
    "\n",
    "* in the virtual environment you should find other discharge .txt files. Ask python to read and analyse your selected input file!\n",
    "* which is the discharge value for Return Period = 5?\n",
    "* which is the discharge value with a Probability of exceedance = 0.5?\n",
    "* can you plot the return-periods in a logaritmic scale (for y)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
